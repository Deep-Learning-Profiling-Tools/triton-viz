name: Sanitizer Benchmark

on:
  pull_request:
    branches:
      - main

permissions:
  contents: read
  pull-requests: write

concurrency:
  group: benchmark-${{ github.head_ref || github.ref }}
  cancel-in-progress: true

jobs:
  benchmark:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout PR branch
      uses: actions/checkout@v4
      with:
        path: pr-branch

    - name: Checkout main branch
      uses: actions/checkout@v4
      with:
        ref: main
        path: main-branch

    - name: Set up Python 3.12
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'

    - name: Install uv
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        echo "$HOME/.cargo/bin" >> $GITHUB_PATH

    - name: Install main dependencies
      continue-on-error: true
      run: |
        cd main-branch
        uv sync --extra test

    - name: Install PR dependencies
      run: |
        cd pr-branch
        uv sync --extra test

    - name: Run interleaved A/B benchmarks
      run: |
        # Run main and PR in alternating rounds so environment drift
        # (thermal throttling, background processes) affects both equally.
        #   main PR main PR main PR ...  (8 rounds x 5 iters = 40 total)
        ROUNDS=8
        ITERS=5
        MAIN_HAS_BENCH=false
        if [ -f main-branch/benchmarks/bench_sanitizer.py ]; then
          MAIN_HAS_BENCH=true
        fi
        for i in $(seq 1 $ROUNDS); do
          echo "=== Round $i/$ROUNDS ==="
          if [ "$MAIN_HAS_BENCH" = true ]; then
            (cd main-branch && uv run python benchmarks/bench_sanitizer.py \
              --iterations $ITERS --output /tmp/bench-main-$i.json) || true
          fi
          (cd pr-branch && uv run python benchmarks/bench_sanitizer.py \
            --iterations $ITERS --output /tmp/bench-pr-$i.json)
        done

    - name: Merge rounds and generate comparison
      run: |
        cd pr-branch
        # Merge per-round results into single files
        if ls /tmp/bench-main-*.json 1>/dev/null 2>&1; then
          uv run python benchmarks/bench_sanitizer.py \
            --merge /tmp/bench-main-*.json -o /tmp/bench-main.json
        fi
        uv run python benchmarks/bench_sanitizer.py \
          --merge /tmp/bench-pr-*.json -o /tmp/bench-pr.json
        # Generate comparison markdown
        if [ -f /tmp/bench-main.json ]; then
          uv run python benchmarks/bench_sanitizer.py \
            --compare /tmp/bench-main.json /tmp/bench-pr.json > /tmp/bench-comment.md
        else
          uv run python benchmarks/bench_sanitizer.py \
            --compare-single /tmp/bench-pr.json > /tmp/bench-comment.md
        fi

    - name: Post or update PR comment
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const body = fs.readFileSync('/tmp/bench-comment.md', 'utf8');
          const marker = '<!-- sanitizer-benchmark -->';
          const fullBody = marker + '\n' + body;

          const { data: comments } = await github.rest.issues.listComments({
            owner: context.repo.owner,
            repo: context.repo.repo,
            issue_number: context.issue.number,
          });

          const existing = comments.find(c => c.body.includes(marker));

          if (existing) {
            await github.rest.issues.updateComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: existing.id,
              body: fullBody,
            });
          } else {
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: fullBody,
            });
          }
